---
layout: post
tags: [Productivity, Unit Testing]
date: 2011-04-29 13:02:14 UTC
title: "Feedback mechanisms and tradeoffs"
description: "The faster we can get feedback, the less it costs to correct errors. Unit testing is only one amongh many sources of rapid feedback."
---
{% include JB/setup %}

<div id="post">
	<p>
		<em>Rapid feedback</em> is one of the cornerstones of agile development. The faster we can get feedback, the less it costs to correct errors. Unit testing is one of the ways to get feedback, but not the only one. Each way we can get feedback comes with its own advantages and disadvantages.
	</p>
	<p>
		In my experience there's a tradeoff between the cost of setting up a feedback mechanism and the level of confidence we can get from it. This is quite intuitive to most people, but I've rarely seen it explicitly described. The purpose of this post is to explicitly describe and relate those different mechanisms.
	</p>
	<h3 id="dcce16b851974ef7b7abd2be9c4214e2">
		Compilation <a href="#dcce16b851974ef7b7abd2be9c4214e2" title="permalink">#</a>
	</h3>
	<p>
		In compiled languages, compilation provides the first level of feedback. I think a lot of people don't think about this as feedback as (for compiled languages) it's a necessary step before the code can be executed.
	</p>
	<p>
		The level of confidence may not be very high  -  we tend to laugh at statements like ‘if it compiles, it works'. However, the compiler still catches a lot of mistakes. Think about it: does your code always compile, or do you sometimes get compilation errors? Do you sometimes try to compile your code just to see if it's possible? I certainly do, and that's because the compiler is always available, and it's the first and fastest level of feedback we get.
	</p>
	<p>
		Code can be designed to take advantage of this verification step. Constructor Injection, for example, ensures that we can't create a new instance of a class without supplying it with its <em>required</em> dependencies.
	</p>
	<p>
		It's also interesting to note that anecdotal evidence seems to suggest that unit testing is more prevalent for interpreted languages. That makes a lot of sense, because as developers we need feedback as soon as possible, and if there's no compiler we must reach for the next level of feedback mechanism.
	</p>
	<h3 id="0a8a0827f14d430bb802e09085af967f">
		Static code analysis <a href="#0a8a0827f14d430bb802e09085af967f" title="permalink">#</a>
	</h3>
	<p>
		The idea of static code analysis isn't specific to .NET, but in certain versions of Visual Studio we have Code Analysis (which is also available as FxCop). Static code analysis is a step up from compilation  -  not only is code checked for syntactical correctness, but it's also checked against a set of known anti-patterns and idioms.
	</p>
	<p>
		Static code analysis is encapsulated expert knowledge, yet surprisingly few people use it. I think part of the reason for this is because the ratio of time against confidence isn't as compelling as with other feedback mechanism. It takes some time to run the analysis, but like compilation the level of confidence gained from getting a clean result is not very high.
	</p>
	<p>
		Personally, I use static code analysis once in a while (e.g. before checking in code to the default branch), but not as often as other feedback mechanisms. Still, I think this type of feedback mechanism is under-utilized.
	</p>
	<h3 id="3f34b7661d584febade7dc802d78decf">
		Unit testing <a href="#3f34b7661d584febade7dc802d78decf" title="permalink">#</a>
	</h3>
	<p>
		Running a unit test suite is one of the most efficient ways to gain rapid feedback. The execution time is often comparable to running static code analysis while the level of confidence is much higher.
	</p>
	<p>
		Obviously a successful unit test execution is no guarantee that the final application works as intended, but it's a much better indicator than the two previous mechanisms. When presented with the choice of using either static code analysis or unit tests, I prefer unit tests every time because the confidence level is much higher.
	</p>
	<p>
		If you have sufficiently high code coverage I'd say that a successful unit test suite is enough confidence to check in code and let continuous integration take care of the rest.
	</p>
	<blockquote>
		<p>
			Keep in mind that continuous integration and other types of automated builds are feedback mechanisms. If you institute rules where people are ‘punished' for checking in code that breaks the build, you effectively disable the automated build as a source of feedback.
		</p>
	</blockquote>
	<p>
		Thus, the rest of these feedback mechanisms should be used rarely (if at all) on developer work stations.
	</p>
	<h3 id="d0d460f264ef41e0b01a4981675c0386">
		Integration testing <a href="#d0d460f264ef41e0b01a4981675c0386" title="permalink">#</a>
	</h3>
	<p>
		Integration testing comes in several sub-categories. You can integrate multiple classes from within the same library, or integrate multiple libraries while still using <a href="http://xunitpatterns.com/Test%20Double.html">Test Doubles</a> instead of out-of-process resources. At this level, the cost/confidence ratio is comparable to unit testing.
	</p>
	<h3 id="90f460b4f36a4a87b1cf2bfe18adc15c">
		Subcutaneous testing <a href="#90f460b4f36a4a87b1cf2bfe18adc15c" title="permalink">#</a>
	</h3>
	<p>
		A more full level of integration testing comes when all resources are integrated, but the tests are still driven by code instead of through the UI. While this gives a degree of confidence that is close to what you can get from a full systems test, the cost of setting up the entire testing environment is <em>much</em> higher. In many cases I consider this the realm of a dedicated testing department, as it's often a full-time job to maintain the suite of automation tools that enable such tests  -  particularly if we are talking about distributed systems.
	</p>
	<h3 id="8906e9d008234b9bab176b40fd76ede6">
		System testing <a href="#8906e9d008234b9bab176b40fd76ede6" title="permalink">#</a>
	</h3>
	<p>
		This is a test of the full system, often driven through the UI and supplemented with manual testing (such as exploratory testing). This provides the highest degree of confidence, but comes with a very high cost. It's often at this level we find formal acceptance tests.
	</p>
	<p>
		The time before we can get feedback at this level is often considerable. It may take days or even weeks or months.
	</p>
	<h3 id="1d76dfee6bdc4433b0fe6e7ce24ed7a8">
		Understand the cost/confidence ratio <a href="#1d76dfee6bdc4433b0fe6e7ce24ed7a8" title="permalink">#</a>
	</h3>
	<p>
		The purpose of this post has been to talk about different ways we can get feedback when developing software. If we could get <em>instantaneous</em> feedback with <em>guaranteed</em> confidence it would be easy to choose, but as this is not the case we must understand the benefits and drawbacks of each mechanism.
	</p>
	<p>
		It's important to realize that there are many mechanisms, and that we can combine them in a staggered approach where we start with fast feedback (like the compiler) with a low degree of confidence and then move through successive stages that each provide a higher level of confidence.
	</p>
</div>
	
<div id="comments">
<hr>
<h2 id="comments-header">
	Comments
</h2>
	<div class="comment" id="355037f6af7644a58dc88cb87d785d03">
	<div class="comment-author">Markus Hj&#228;rne</div>
	<div class="comment-content">Hi Mark, good post. I just thought of one thing worth mentioning in this context: guarding public APIs with checks that throws exceptions in case they fail and guarding private methods with asserts has been a great source of good and fast feedback for me over the years.<br>
<br>
Regards,<br>
Markus</div>
	<div class="comment-date">2011-05-04 20:46 UTC</div>
</div>
<div class="comment" id="6dc4e5d1bf2a40dd8ed59805b88a2af8">
	<div class="comment-author"><a href="{{ HOME_PATH }}">Mark Seemann</a></div>
	<div class="comment-content">I totally agree, and I was (and am still) planning on writing something about that in a later post. This is essentially the Fail Fast principle, but as for feedback mechanisms, we don't discover these Guard Clauses before we invoke them.<br>
<br>
My strong preference is for writing unit tests that invoke these Guard Clauses to verify that they work, and that means that as far as feedback mechanisms go, they fall into the unit testing 'bucket'.</div>
	<div class="comment-date">2011-05-04 20:54 UTC</div>
</div>
<div class="comment" id="bae47385fd724c11b71442f3eeb7e28a">
	<div class="comment-author"><a href="http://gogllundain.blogspot.co.uk/">GogLlundain</a></div>
	<div class="comment-content">Your comments about compilation are interesting, I hadn't thought of it before but it is a level of feedback that I find as useful as you do.<br>
<br>
I wonder if you'd consider IntelliSense as an even earlier form of feedback though?<br>
<br>
Gog</div>
	<div class="comment-date">2012-06-25 09:20 UTC</div>
</div>
<div class="comment" id="8fa137df8ef842d98fc2ccf4c09b6e3a">
	<div class="comment-author"><a href="{{ HOME_PATH }}">Mark Seemann</a></div>
	<div class="comment-content">IntelliSense is, I think, more of a learning tool. It's not really <em>feedback</em> about what you <em>did</em> - it's more <em>information</em> about what you <em>can do</em>. It's certainly a productivity feature, although there are some people who argue that it does more harm than good.</div>
	<div class="comment-date">2012-06-25 10:50 UTC</div>
</div>


<div class="comment" id="3c270099afdf4c4386c4fc5af6b2c33a">
    <div class="comment-author"><a href="https://codevergnugen.wordpress.com/">David Falkner</a></div>
    <div class="comment-content">It might be worth considering a couple of additional feedback mechanisms at the opposite end of the spectrum, in the sense that they provide even later feedback, if only to underscore your points from the article with extreme counter-examples.  <em>Run-time exceptions</em> are themselves a feedback mechanism.  Most unnecessarily "stringly-typed" code could be considered to be pushing code away from the more rapid, desirable feedback mechanisms.  (Primitive Obsession in general might be considered another tendency which pushes the design of code toward being run-time exception-prone.)  <em>Logic erors</em> are flaws in code with which the code appears to execute successfully, until the program output is manually inspected.  Just off the cuff, one typical cause might be the tendency in some projects to add guard clauses such as <em>if (methodArg==null) return;</em> which causes methods to silently and unexpectedly return without accomplishing the task that they were designed to do.  Mathematical calculations performed without adequate test coverage might be another example.<br/><br/>We learned about 3 classes of errors in school: 1) Syntax, 2) Run-time, and 3) Logic (Unit testing and static code analysis weren't as much of a thing back then).  It might seem odd to discuss types of feedback mechanisms that are undesirable by definition, until you stop to observe how common it is for the design of much code to push errors <em>towards</em> this undesirable side of the continuum.  "To defeat your enemy you first must know him" or something like that.</div>
    <div class="comment-date">2016-09-11 00:11 UTC</div>
</div>

	<div class="comment" id="556888cbfd2141569378d1b90abaa77e">
		<div class="comment-author"><a href="{{ HOME_PATH }}">Mark Seemann</a></div>
		<div class="comment-content">
			<p>
				David, thank you for making these feedback cases explicit. When I originally wrote this article, I implicitly had such alternative, common-place feedback mechanisms in mind, but I never explicitly spelled them out. While I should have done that, I can now thank you for doing it; your comment is a good addition to the original article.
			</p>
		</div>
		<div class="comment-date">2016-09-11 08:07 UTC</div>
	</div>
</div>